---
title: "Machine Learning from A to Z: Part 6 - Unsupervised Learning"
description: "Master unsupervised learning: K-Means, DBSCAN, PCA, t-SNE, and more. Learn clustering, dimensionality reduction, and anomaly detection with Python."
date: "2025-10-25"
author: "Goutham"
tags: ["Machine Learning", "Unsupervised Learning", "Clustering", "PCA", "K-Means"]
image: "/images/ml-unsupervised.svg"
readTime: "23 min read"
---

# Machine Learning from A to Z: Part 6 - Unsupervised Learning

Discover hidden patterns! Master clustering, dimensionality reduction, and anomaly detection.

## What is Unsupervised Learning?

**Unsupervised Learning** finds patterns in data **without labels**.

```python
# Supervised: You have labels
X = [[features], ...]
y = [labels]  # Known categories

# Unsupervised: No labels
X = [[features], ...]
# No y! Find patterns automatically
```

## Types of Unsupervised Learning

```
Unsupervised Learning
‚îú‚îÄ‚îÄ Clustering (Group similar items)
‚îÇ   ‚îú‚îÄ‚îÄ K-Means
‚îÇ   ‚îú‚îÄ‚îÄ DBSCAN
‚îÇ   ‚îî‚îÄ‚îÄ Hierarchical
‚îú‚îÄ‚îÄ Dimensionality Reduction (Simplify data)
‚îÇ   ‚îú‚îÄ‚îÄ PCA
‚îÇ   ‚îú‚îÄ‚îÄ t-SNE
‚îÇ   ‚îî‚îÄ‚îÄ UMAP
‚îî‚îÄ‚îÄ Anomaly Detection (Find outliers)
    ‚îî‚îÄ‚îÄ Isolation Forest
```

## K-Means Clustering

Group data into K clusters.

### Basic K-Means

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generate sample data
X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=42)

# Train K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

# Get cluster centers
centers = kmeans.cluster_centers_

# Visualize
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)
plt.scatter(centers[:, 0], centers[:, 1],
           c='red', marker='X', s=200, edgecolors='black',
           label='Centroids')
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Predict cluster for new point
new_point = [[0, 0]]
cluster = kmeans.predict(new_point)
print(f"New point belongs to cluster: {cluster[0]}")
```

### Finding Optimal K (Elbow Method)

```python
# Try different values of K
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)  # Sum of squared distances

# Plot elbow curve
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia (Within-cluster sum of squares)')
plt.title('Elbow Method for Optimal K')
plt.grid(True)
plt.show()

print("Look for the 'elbow' - where inertia stops decreasing sharply")
```

### Silhouette Score

```python
from sklearn.metrics import silhouette_score

# Calculate silhouette scores
silhouette_scores = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f"K={k}: Silhouette Score = {score:.3f}")

# Plot
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Different K')
plt.grid(True)
plt.show()

# Higher silhouette score = better clustering
optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
print(f"\nOptimal K: {optimal_k}")
```

## Real-World Example: Customer Segmentation

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Create customer data
np.random.seed(42)
n = 500

data = pd.DataFrame({
    'age': np.random.randint(18, 70, n),
    'annual_income': np.random.randint(15, 150, n),  # in thousands
    'spending_score': np.random.randint(1, 100, n),   # 1-100
    'loyalty_years': np.random.randint(0, 20, n)
})

print("Customer Data:")
print(data.head())
print(f"\nShape: {data.shape}")

# Standardize features (important for K-Means!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data)

# Find optimal K
inertias = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

plt.plot(range(1, 11), inertias, marker='o')
plt.xlabel('K')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

# Apply K-Means with K=4
kmeans = KMeans(n_clusters=4, random_state=42)
data['cluster'] = kmeans.fit_predict(X_scaled)

# Analyze clusters
print("\nCluster Statistics:")
cluster_stats = data.groupby('cluster').mean()
print(cluster_stats)

# Visualize (2D projection)
plt.figure(figsize=(12, 5))

# Income vs Spending
plt.subplot(1, 2, 1)
for cluster in range(4):
    cluster_data = data[data['cluster'] == cluster]
    plt.scatter(cluster_data['annual_income'],
               cluster_data['spending_score'],
               label=f'Cluster {cluster}',
               alpha=0.6)
plt.xlabel('Annual Income ($k)')
plt.ylabel('Spending Score')
plt.title('Customer Segments: Income vs Spending')
plt.legend()

# Age vs Loyalty
plt.subplot(1, 2, 2)
for cluster in range(4):
    cluster_data = data[data['cluster'] == cluster]
    plt.scatter(cluster_data['age'],
               cluster_data['loyalty_years'],
               label=f'Cluster {cluster}',
               alpha=0.6)
plt.xlabel('Age')
plt.ylabel('Loyalty (years)')
plt.title('Customer Segments: Age vs Loyalty')
plt.legend()

plt.tight_layout()
plt.show()

# Interpret clusters
print("\nCluster Interpretation:")
for cluster in range(4):
    cluster_data = data[data['cluster'] == cluster]
    avg_income = cluster_data['annual_income'].mean()
    avg_spending = cluster_data['spending_score'].mean()
    avg_age = cluster_data['age'].mean()

    print(f"\nCluster {cluster} ({len(cluster_data)} customers):")
    print(f"  Average Income: ${avg_income:.0f}k")
    print(f"  Average Spending Score: {avg_spending:.0f}")
    print(f"  Average Age: {avg_age:.0f}")

    if avg_income > 100 and avg_spending > 60:
        print("  Type: Premium customers (high value)")
    elif avg_income < 50 and avg_spending > 60:
        print("  Type: Impulsive buyers (retention target)")
    elif avg_income > 100 and avg_spending < 40:
        print("  Type: Careful spenders (upsell potential)")
    else:
        print("  Type: Budget-conscious (value deals)")
```

## DBSCAN (Density-Based Clustering)

Finds clusters of arbitrary shapes, identifies outliers.

```python
from sklearn.cluster import DBSCAN

# Generate data with irregular shapes
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=300, noise=0.05, random_state=42)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels = dbscan.fit_predict(X)

# -1 = noise/outlier, 0, 1, 2... = clusters
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"Number of clusters: {n_clusters}")
print(f"Number of noise points: {n_noise}")

# Visualize
plt.figure(figsize=(10, 6))
unique_labels = set(labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

for label, color in zip(unique_labels, colors):
    if label == -1:
        # Noise points
        color = 'black'
        marker = 'x'
    else:
        marker = 'o'

    class_member_mask = (labels == label)
    xy = X[class_member_mask]
    plt.scatter(xy[:, 0], xy[:, 1], c=[color], marker=marker,
               s=50, alpha=0.6, label=f'Cluster {label}' if label != -1 else 'Noise')

plt.title('DBSCAN Clustering')
plt.legend()
plt.show()
```

### K-Means vs DBSCAN

```python
# Compare on same data
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
labels_kmeans = kmeans.fit_predict(X)
axes[0].scatter(X[:, 0], X[:, 1], c=labels_kmeans, cmap='viridis')
axes[0].set_title('K-Means (struggles with shapes)')

# DBSCAN
labels_dbscan = dbscan.fit_predict(X)
axes[1].scatter(X[:, 0], X[:, 1], c=labels_dbscan, cmap='viridis')
axes[1].set_title('DBSCAN (handles shapes well)')

plt.show()

# When to use each?
print("K-Means:")
print("‚úÖ Know number of clusters")
print("‚úÖ Spherical/circular clusters")
print("‚úÖ Fast on large datasets")

print("\nDBSCAN:")
print("‚úÖ Arbitrary cluster shapes")
print("‚úÖ Unknown number of clusters")
print("‚úÖ Need to identify outliers")
```

## Hierarchical Clustering

Builds tree of clusters.

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# Sample data
X = np.array([[1, 2], [2, 3], [3, 1], [8, 7], [9, 8], [10, 7]])

# Compute linkage
Z = linkage(X, method='ward')

# Plot dendrogram
plt.figure(figsize=(10, 6))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Cut tree to get clusters
agg_clustering = AgglomerativeClustering(n_clusters=2)
labels = agg_clustering.fit_predict(X)

plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=100)
plt.title('Hierarchical Clustering Result')
plt.show()
```

## Principal Component Analysis (PCA)

Reduce dimensions while preserving variance.

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import pandas as pd

# Load iris dataset (4 features)
iris = load_iris()
X = iris.data
y = iris.target

print(f"Original shape: {X.shape}")  # 150 samples, 4 features

# Apply PCA to reduce to 2 dimensions
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"Reduced shape: {X_pca.shape}")  # 150 samples, 2 features

# Explained variance
print(f"\nExplained variance ratio: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}")

# Visualize
plt.figure(figsize=(10, 6))
colors = ['red', 'green', 'blue']
for i, color in enumerate(colors):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],
               c=color, label=iris.target_names[i], alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('PCA: Iris Dataset (4D ‚Üí 2D)')
plt.legend()
plt.grid(True)
plt.show()
```

### Choosing Number of Components

```python
# Fit PCA with all components
pca_full = PCA()
pca_full.fit(X)

# Plot cumulative explained variance
cumsum = np.cumsum(pca_full.explained_variance_ratio_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumsum) + 1), cumsum, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA: Choosing Number of Components')
plt.legend()
plt.grid(True)
plt.show()

# Find components needed for 95% variance
n_components_95 = np.argmax(cumsum >= 0.95) + 1
print(f"Components needed for 95% variance: {n_components_95}")
```

## t-SNE (t-Distributed Stochastic Neighbor Embedding)

Better visualization for high-dimensional data.

```python
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits

# Load digits dataset (64 features)
digits = load_digits()
X = digits.data
y = digits.target

print(f"Original shape: {X.shape}")  # 1797 samples, 64 features

# Apply t-SNE (slow on large datasets!)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X)

# Visualize
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],
                     c=y, cmap='tab10', alpha=0.6)
plt.colorbar(scatter, label='Digit')
plt.title('t-SNE: Handwritten Digits (64D ‚Üí 2D)')
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.show()

# Compare PCA vs t-SNE
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)
axes[0].set_title('PCA (Linear)')

# t-SNE
axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.6)
axes[1].set_title('t-SNE (Non-linear)')

plt.show()

print("t-SNE better preserves local structure (similar points stay close)")
```

## Anomaly Detection

Find unusual patterns in data.

```python
from sklearn.ensemble import IsolationForest

# Generate normal data + anomalies
np.random.seed(42)
X_normal = np.random.normal(0, 1, (200, 2))
X_anomalies = np.random.uniform(-4, 4, (20, 2))
X = np.vstack([X_normal, X_anomalies])

# Train Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
predictions = iso_forest.fit_predict(X)

# -1 = anomaly, 1 = normal
n_anomalies = list(predictions).count(-1)
print(f"Detected anomalies: {n_anomalies}")

# Visualize
plt.figure(figsize=(10, 6))
plt.scatter(X[predictions == 1, 0], X[predictions == 1, 1],
           c='blue', label='Normal', alpha=0.6)
plt.scatter(X[predictions == -1, 0], X[predictions == -1, 1],
           c='red', label='Anomaly', alpha=0.8, marker='x', s=100)
plt.title('Anomaly Detection with Isolation Forest')
plt.legend()
plt.grid(True)
plt.show()
```

## Real-World Example: Image Compression with PCA

```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load face images
faces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X = faces.data
n_samples, n_features = X.shape

print(f"Dataset: {n_samples} images, {n_features} pixels each")

# Original image
sample_image = X[0].reshape(50, 37)

# Compress with different numbers of components
components_list = [10, 50, 150, 400]

fig, axes = plt.subplots(1, len(components_list) + 1, figsize=(15, 3))

# Original
axes[0].imshow(sample_image, cmap='gray')
axes[0].set_title('Original\n(1850 pixels)')
axes[0].axis('off')

# Compressed versions
for i, n_comp in enumerate(components_list):
    pca = PCA(n_components=n_comp)
    X_compressed = pca.fit_transform(X)
    X_reconstructed = pca.inverse_transform(X_compressed)

    compressed_image = X_reconstructed[0].reshape(50, 37)
    variance = pca.explained_variance_ratio_.sum()

    axes[i + 1].imshow(compressed_image, cmap='gray')
    axes[i + 1].set_title(f'{n_comp} components\n({variance:.1%} variance)')
    axes[i + 1].axis('off')

plt.tight_layout()
plt.show()

# Compression ratio
original_size = n_features
for n_comp in components_list:
    compressed_size = n_comp
    ratio = original_size / compressed_size
    print(f"{n_comp} components: {ratio:.1f}x compression")
```

## Practice Exercises

### Exercise 1: Mall Customer Segmentation

```python
# Task: Segment mall customers
data = pd.DataFrame({
    'age': [19, 21, 20, 23, 31, 22, 35, 23, 64, 30],
    'annual_income': [15, 15, 16, 16, 17, 17, 18, 18, 19, 19],
    'spending_score': [39, 81, 6, 77, 40, 76, 6, 94, 3, 72]
})

# Your tasks:
# 1. Standardize the data
# 2. Find optimal K using elbow method
# 3. Apply K-Means
# 4. Visualize clusters
# 5. Interpret each cluster
```

### Exercise 2: Dimensionality Reduction

```python
from sklearn.datasets import fetch_openml

# Load MNIST (70k images, 784 pixels each)
mnist = fetch_openml('mnist_784', version=1, parser='auto')
X = mnist.data[:1000]  # Use subset for speed

# Your tasks:
# 1. Apply PCA to reduce to 50 components
# 2. Calculate variance explained
# 3. Visualize first 2 principal components
# 4. Compare with t-SNE visualization
```

### Exercise 3: Anomaly Detection

```python
# Detect fraudulent transactions
transactions = pd.DataFrame({
    'amount': np.concatenate([
        np.random.normal(50, 20, 95),  # Normal
        np.random.uniform(500, 1000, 5)  # Fraud
    ]),
    'location_distance': np.concatenate([
        np.random.normal(5, 2, 95),
        np.random.uniform(100, 500, 5)
    ])
})

# Your tasks:
# 1. Apply Isolation Forest
# 2. Identify anomalies
# 3. Visualize results
```

## Interview Questions

**Q1: When to use unsupervised vs supervised learning?**
- **Unsupervised**: No labels, want to find patterns (customer segmentation, anomaly detection)
- **Supervised**: Have labels, want to predict (spam detection, price prediction)

**Q2: How does K-Means work?**
1. Initialize K random centroids
2. Assign each point to nearest centroid
3. Update centroids to mean of assigned points
4. Repeat steps 2-3 until convergence

**Q3: What is the "elbow" in elbow method?**
Point where adding more clusters doesn't significantly reduce inertia. Indicates optimal K.

**Q4: PCA vs t-SNE - when to use each?**
- **PCA**: Fast, linear, preserves global structure, interpretable
- **t-SNE**: Slow, non-linear, preserves local structure, visualization only

**Q5: How does DBSCAN differ from K-Means?**
```python
K-Means:
‚úÖ Need to specify K
‚úÖ Assumes spherical clusters
‚úÖ Fast
‚ùå Can't handle irregular shapes
‚ùå Sensitive to outliers

DBSCAN:
‚úÖ Automatically finds number of clusters
‚úÖ Handles arbitrary shapes
‚úÖ Identifies outliers
‚ùå Sensitive to parameters (eps, min_samples)
‚ùå Struggles with varying densities
```

**Q6: What is curse of dimensionality?**
As dimensions increase:
- Distance metrics become less meaningful
- Data becomes sparse
- Models need exponentially more data
- Solution: Dimensionality reduction (PCA, feature selection)

## Key Takeaways

‚úÖ **K-Means**: Fast, simple, need to specify K
‚úÖ **DBSCAN**: Handles shapes, finds outliers
‚úÖ **Hierarchical**: Creates tree, no need to specify K
‚úÖ **PCA**: Fast dimensionality reduction, linear
‚úÖ **t-SNE**: Better visualization, non-linear, slow
‚úÖ **Always standardize features** for clustering
‚úÖ **Use elbow method or silhouette score** to find optimal K

**Part 7** covers model evaluation and hyperparameter tuning! üöÄ

---

*Series Progress: 6/8 - Next up: Model Evaluation and Tuning!*
