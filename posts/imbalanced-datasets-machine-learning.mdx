---
title: "Handling Imbalanced Datasets in Machine Learning"
description: "Master techniques for dealing with imbalanced data including SMOTE, undersampling, oversampling, and cost-sensitive learning with real-world examples and implementations."
date: "2024-12-06"
author: "Tech Blogger"
tags: ["Machine Learning", "Imbalanced Data", "Data Science"]
image: "/images/imbalanced-datasets.jpg"
readTime: "9 min read"
---

# Handling Imbalanced Datasets in Machine Learning

Imbalanced datasets are one of the most common challenges in real-world machine learning. When one class significantly outnumbers others, traditional algorithms often struggle to learn meaningful patterns from minority classes. This comprehensive guide explores advanced techniques to handle class imbalance effectively.

## Understanding Class Imbalance

Class imbalance occurs when the distribution of target classes is not uniform. This is extremely common in real-world scenarios:

```python
# Examples of imbalanced datasets
real_world_examples = {
    'Fraud Detection': '0.1% fraudulent transactions',
    'Medical Diagnosis': '5% positive cases',
    'Email Spam': '10% spam messages',
    'Customer Churn': '15% churned customers',
    'Manufacturing Defects': '2% defective products',
    'Click-through Rates': '1% click rate',
    'Disease Screening': '0.5% positive cases'
}

# The problem with traditional accuracy
total_samples = 1000
positive_class = 50    # 5% minority class
negative_class = 950   # 95% majority class

# A "dumb" classifier that always predicts majority class
accuracy = negative_class / total_samples  # 95% accuracy!
# But it completely fails to identify the important minority class
```

## Why Standard Algorithms Struggle

Traditional machine learning algorithms make assumptions that break down with imbalanced data:

```
ðŸš« Problems with Imbalanced Data:
â”œâ”€â”€ ðŸ“Š Accuracy Paradox
â”‚   â””â”€â”€ High accuracy by predicting majority class only
â”œâ”€â”€ ðŸŽ¯ Biased Learning
â”‚   â””â”€â”€ Algorithms optimize for overall accuracy
â”œâ”€â”€ ðŸ“‰ Poor Minority Detection
â”‚   â””â”€â”€ Low recall for important minority classes
â””â”€â”€ âš–ï¸ Cost Ignorance
    â””â”€â”€ All misclassifications treated equally
```

Let's explore comprehensive solutions:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                           precision_recall_curve, roc_curve, f1_score, precision_score,
                           recall_score, accuracy_score, balanced_accuracy_score)
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')

# Create imbalanced dataset
X, y = make_classification(
    n_samples=10000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_clusters_per_class=1,
    weights=[0.95, 0.05],  # 95% majority, 5% minority
    random_state=42
)

print(f"Dataset shape: {X.shape}")
print(f"Class distribution: {np.bincount(y)}")
print(f"Minority class percentage: {np.bincount(y)[1]/len(y)*100:.2f}%")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Visualize class imbalance
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.bar(['Majority (0)', 'Minority (1)'], np.bincount(y), color=['skyblue', 'coral'])
plt.title('Full Dataset Class Distribution')
plt.ylabel('Count')

plt.subplot(1, 3, 2)
plt.bar(['Majority (0)', 'Minority (1)'], np.bincount(y_train), color=['skyblue', 'coral'])
plt.title('Training Set Class Distribution')
plt.ylabel('Count')

plt.subplot(1, 3, 3)
plt.bar(['Majority (0)', 'Minority (1)'], np.bincount(y_test), color=['skyblue', 'coral'])
plt.title('Test Set Class Distribution')
plt.ylabel('Count')

plt.tight_layout()
plt.show()
```

## Evaluation Metrics for Imbalanced Data

Before diving into solutions, we need proper evaluation metrics:

### Comprehensive Evaluation Framework

```python
class ImbalancedEvaluator:
    """Comprehensive evaluation for imbalanced datasets"""

    def __init__(self, y_true, y_pred, y_prob=None):
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_prob = y_prob

    def compute_all_metrics(self):
        """Compute all relevant metrics for imbalanced classification"""

        metrics = {}

        # Basic metrics
        metrics['accuracy'] = accuracy_score(self.y_true, self.y_pred)
        metrics['balanced_accuracy'] = balanced_accuracy_score(self.y_true, self.y_pred)

        # Class-specific metrics
        metrics['precision'] = precision_score(self.y_true, self.y_pred, average='binary')
        metrics['recall'] = recall_score(self.y_true, self.y_pred, average='binary')
        metrics['f1'] = f1_score(self.y_true, self.y_pred, average='binary')

        # Macro averages (equal weight to all classes)
        metrics['precision_macro'] = precision_score(self.y_true, self.y_pred, average='macro')
        metrics['recall_macro'] = recall_score(self.y_true, self.y_pred, average='macro')
        metrics['f1_macro'] = f1_score(self.y_true, self.y_pred, average='macro')

        # Weighted averages (weight by class frequency)
        metrics['precision_weighted'] = precision_score(self.y_true, self.y_pred, average='weighted')
        metrics['recall_weighted'] = recall_score(self.y_true, self.y_pred, average='weighted')
        metrics['f1_weighted'] = f1_score(self.y_true, self.y_pred, average='weighted')

        # Confusion matrix metrics
        tn, fp, fn, tp = confusion_matrix(self.y_true, self.y_pred).ravel()

        metrics['sensitivity'] = tp / (tp + fn)  # Same as recall
        metrics['specificity'] = tn / (tn + fp)
        metrics['false_positive_rate'] = fp / (fp + tn)
        metrics['false_negative_rate'] = fn / (fn + tp)

        # Geometric mean
        metrics['g_mean'] = np.sqrt(metrics['sensitivity'] * metrics['specificity'])

        # Matthews Correlation Coefficient
        mcc_num = (tp * tn) - (fp * fn)
        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        metrics['mcc'] = mcc_num / mcc_den if mcc_den != 0 else 0

        # AUC metrics (if probabilities available)
        if self.y_prob is not None:
            metrics['auc_roc'] = roc_auc_score(self.y_true, self.y_prob)

            # Precision-Recall AUC
            precision_curve, recall_curve, _ = precision_recall_curve(self.y_true, self.y_prob)
            metrics['auc_pr'] = np.trapz(precision_curve, recall_curve)

        return metrics

    def print_detailed_report(self):
        """Print comprehensive evaluation report"""

        metrics = self.compute_all_metrics()

        print("IMBALANCED CLASSIFICATION REPORT")
        print("=" * 50)

        print(f"\nðŸ“Š Basic Metrics:")
        print(f"   Accuracy: {metrics['accuracy']:.4f}")
        print(f"   Balanced Accuracy: {metrics['balanced_accuracy']:.4f}")

        print(f"\nðŸŽ¯ Minority Class (Class 1) Performance:")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall (Sensitivity): {metrics['recall']:.4f}")
        print(f"   F1-Score: {metrics['f1']:.4f}")

        print(f"\nâš–ï¸ Class Balance Metrics:")
        print(f"   Specificity: {metrics['specificity']:.4f}")
        print(f"   G-Mean: {metrics['g_mean']:.4f}")
        print(f"   MCC: {metrics['mcc']:.4f}")

        if 'auc_roc' in metrics:
            print(f"\nðŸ“ˆ AUC Metrics:")
            print(f"   ROC-AUC: {metrics['auc_roc']:.4f}")
            print(f"   PR-AUC: {metrics['auc_pr']:.4f}")

        print(f"\nðŸ” Error Analysis:")
        cm = confusion_matrix(self.y_true, self.y_pred)
        tn, fp, fn, tp = cm.ravel()
        print(f"   True Negatives: {tn}")
        print(f"   False Positives: {fp}")
        print(f"   False Negatives: {fn}")
        print(f"   True Positives: {tp}")
        print(f"   FPR: {metrics['false_positive_rate']:.4f}")
        print(f"   FNR: {metrics['false_negative_rate']:.4f}")

        return metrics

    def plot_evaluation_charts(self):
        """Create comprehensive evaluation visualizations"""

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. Confusion Matrix
        cm = confusion_matrix(self.y_true, self.y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('Confusion Matrix')
        axes[0, 0].set_ylabel('True Label')
        axes[0, 0].set_xlabel('Predicted Label')

        # 2. ROC Curve (if probabilities available)
        if self.y_prob is not None:
            fpr, tpr, _ = roc_curve(self.y_true, self.y_prob)
            auc_score = roc_auc_score(self.y_true, self.y_prob)

            axes[0, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')
            axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)
            axes[0, 1].set_xlabel('False Positive Rate')
            axes[0, 1].set_ylabel('True Positive Rate')
            axes[0, 1].set_title('ROC Curve')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. Precision-Recall Curve (if probabilities available)
        if self.y_prob is not None:
            precision_curve, recall_curve, _ = precision_recall_curve(self.y_true, self.y_prob)
            pr_auc = np.trapz(precision_curve, recall_curve)

            axes[0, 2].plot(recall_curve, precision_curve, label=f'PR Curve (AUC = {pr_auc:.3f})')
            axes[0, 2].set_xlabel('Recall')
            axes[0, 2].set_ylabel('Precision')
            axes[0, 2].set_title('Precision-Recall Curve')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)

        # 4. Class Distribution
        unique, counts = np.unique(self.y_true, return_counts=True)
        axes[1, 0].bar(['Majority (0)', 'Minority (1)'], counts, color=['skyblue', 'coral'])
        axes[1, 0].set_title('True Class Distribution')
        axes[1, 0].set_ylabel('Count')

        # 5. Prediction Distribution
        unique_pred, counts_pred = np.unique(self.y_pred, return_counts=True)
        if len(unique_pred) == 2:
            axes[1, 1].bar(['Predicted 0', 'Predicted 1'], counts_pred, color=['lightblue', 'salmon'])
        else:
            # Handle case where model predicts only one class
            pred_counts = [0, 0]
            for i, pred_class in enumerate(unique_pred):
                pred_counts[pred_class] = counts_pred[i]
            axes[1, 1].bar(['Predicted 0', 'Predicted 1'], pred_counts, color=['lightblue', 'salmon'])

        axes[1, 1].set_title('Predicted Class Distribution')
        axes[1, 1].set_ylabel('Count')

        # 6. Metrics Comparison
        metrics = self.compute_all_metrics()
        key_metrics = ['precision', 'recall', 'f1', 'balanced_accuracy', 'g_mean']
        metric_values = [metrics[m] for m in key_metrics]

        bars = axes[1, 2].bar(key_metrics, metric_values)
        axes[1, 2].set_ylabel('Score')
        axes[1, 2].set_title('Key Performance Metrics')
        axes[1, 2].set_ylim(0, 1)

        # Color bars based on performance
        for bar, value in zip(bars, metric_values):
            if value >= 0.8:
                bar.set_color('green')
            elif value >= 0.6:
                bar.set_color('orange')
            else:
                bar.set_color('red')

        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# Test baseline model to see the problem
baseline_rf = RandomForestClassifier(n_estimators=100, random_state=42)
baseline_rf.fit(X_train, y_train)
y_pred_baseline = baseline_rf.predict(X_test)
y_prob_baseline = baseline_rf.predict_proba(X_test)[:, 1]

print("BASELINE MODEL PERFORMANCE:")
evaluator = ImbalancedEvaluator(y_test, y_pred_baseline, y_prob_baseline)
baseline_metrics = evaluator.print_detailed_report()
evaluator.plot_evaluation_charts()
```

## Resampling Techniques

Resampling approaches modify the training data distribution to balance classes:

### Undersampling Methods

```python
class UndersamplingMethods:
    """Various undersampling techniques for imbalanced data"""

    def __init__(self):
        self.resampled_data = {}

    def random_undersampling(self, X, y, random_state=42):
        """Random undersampling of majority class"""

        # Separate classes
        majority_class = 0
        minority_class = 1

        majority_indices = np.where(y == majority_class)[0]
        minority_indices = np.where(y == minority_class)[0]

        # Randomly sample majority class to match minority class size
        np.random.seed(random_state)
        majority_downsampled = np.random.choice(
            majority_indices,
            size=len(minority_indices),
            replace=False
        )

        # Combine downsampled majority with all minority
        undersampled_indices = np.concatenate([majority_downsampled, minority_indices])

        X_resampled = X[undersampled_indices]
        y_resampled = y[undersampled_indices]

        print(f"Original distribution: {np.bincount(y)}")
        print(f"After undersampling: {np.bincount(y_resampled)}")

        return X_resampled, y_resampled

    def tomek_links_undersampling(self, X, y):
        """Tomek Links undersampling (requires imblearn)"""

        try:
            from imblearn.under_sampling import TomekLinks

            tomek = TomekLinks()
            X_resampled, y_resampled = tomek.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After Tomek Links: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Install with: pip install imbalanced-learn")
            return self.random_undersampling(X, y)

    def edited_nearest_neighbours(self, X, y):
        """Edited Nearest Neighbours undersampling"""

        try:
            from imblearn.under_sampling import EditedNearestNeighbours

            enn = EditedNearestNeighbours()
            X_resampled, y_resampled = enn.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After ENN: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Install with: pip install imbalanced-learn")
            return self.random_undersampling(X, y)

    def cluster_centroids_undersampling(self, X, y):
        """Cluster Centroids undersampling"""

        try:
            from imblearn.under_sampling import ClusterCentroids

            cc = ClusterCentroids(random_state=42)
            X_resampled, y_resampled = cc.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After Cluster Centroids: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Install with: pip install imbalanced-learn")
            return self.random_undersampling(X, y)

# Test undersampling methods
undersampler = UndersamplingMethods()

print("=== RANDOM UNDERSAMPLING ===")
X_train_rus, y_train_rus = undersampler.random_undersampling(X_train, y_train)

# Train and evaluate model with undersampled data
rf_rus = RandomForestClassifier(n_estimators=100, random_state=42)
rf_rus.fit(X_train_rus, y_train_rus)
y_pred_rus = rf_rus.predict(X_test)
y_prob_rus = rf_rus.predict_proba(X_test)[:, 1]

print("\nRandom Undersampling Results:")
evaluator_rus = ImbalancedEvaluator(y_test, y_pred_rus, y_prob_rus)
rus_metrics = evaluator_rus.print_detailed_report()
```

### Oversampling Methods

```python
class OversamplingMethods:
    """Various oversampling techniques for imbalanced data"""

    def __init__(self):
        pass

    def random_oversampling(self, X, y, random_state=42):
        """Random oversampling of minority class"""

        majority_class = 0
        minority_class = 1

        majority_indices = np.where(y == majority_class)[0]
        minority_indices = np.where(y == minority_class)[0]

        # Randomly oversample minority class to match majority class size
        np.random.seed(random_state)
        minority_oversampled = np.random.choice(
            minority_indices,
            size=len(majority_indices) - len(minority_indices),
            replace=True
        )

        # Combine original minority with oversampled minority and all majority
        oversampled_indices = np.concatenate([majority_indices, minority_indices, minority_oversampled])

        X_resampled = X[oversampled_indices]
        y_resampled = y[oversampled_indices]

        print(f"Original distribution: {np.bincount(y)}")
        print(f"After oversampling: {np.bincount(y_resampled)}")

        return X_resampled, y_resampled

    def smote_oversampling(self, X, y):
        """SMOTE (Synthetic Minority Oversampling Technique)"""

        try:
            from imblearn.over_sampling import SMOTE

            smote = SMOTE(random_state=42)
            X_resampled, y_resampled = smote.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After SMOTE: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Using random oversampling instead")
            return self.random_oversampling(X, y)

    def borderline_smote(self, X, y):
        """Borderline SMOTE - focuses on borderline minority samples"""

        try:
            from imblearn.over_sampling import BorderlineSMOTE

            borderline_smote = BorderlineSMOTE(random_state=42)
            X_resampled, y_resampled = borderline_smote.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After Borderline SMOTE: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Using SMOTE instead")
            return self.smote_oversampling(X, y)

    def adasyn_oversampling(self, X, y):
        """ADASYN (Adaptive Synthetic Sampling)"""

        try:
            from imblearn.over_sampling import ADASYN

            adasyn = ADASYN(random_state=42)
            X_resampled, y_resampled = adasyn.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After ADASYN: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Using SMOTE instead")
            return self.smote_oversampling(X, y)

    def visualize_synthetic_samples(self, X_original, y_original, X_resampled, y_resampled):
        """Visualize original vs synthetic samples (2D projection)"""

        from sklearn.decomposition import PCA

        # Reduce to 2D for visualization
        pca = PCA(n_components=2, random_state=42)
        X_orig_2d = pca.fit_transform(X_original)
        X_resamp_2d = pca.transform(X_resampled)

        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        # Original data
        minority_orig = X_orig_2d[y_original == 1]
        majority_orig = X_orig_2d[y_original == 0]

        axes[0].scatter(majority_orig[:, 0], majority_orig[:, 1],
                       c='skyblue', alpha=0.6, label='Majority Class')
        axes[0].scatter(minority_orig[:, 0], minority_orig[:, 1],
                       c='coral', alpha=0.8, label='Minority Class')
        axes[0].set_title('Original Data Distribution')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Resampled data
        minority_resamp = X_resamp_2d[y_resampled == 1]
        majority_resamp = X_resamp_2d[y_resampled == 0]

        axes[1].scatter(majority_resamp[:, 0], majority_resamp[:, 1],
                       c='skyblue', alpha=0.6, label='Majority Class')
        axes[1].scatter(minority_resamp[:, 0], minority_resamp[:, 1],
                       c='coral', alpha=0.6, label='Minority Class (inc. synthetic)')
        axes[1].set_title('After Oversampling')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

# Test oversampling methods
oversampler = OversamplingMethods()

print("=== SMOTE OVERSAMPLING ===")
X_train_smote, y_train_smote = oversampler.smote_oversampling(X_train, y_train)

# Visualize synthetic samples
oversampler.visualize_synthetic_samples(X_train, y_train, X_train_smote, y_train_smote)

# Train and evaluate model with SMOTE data
rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)
rf_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = rf_smote.predict(X_test)
y_prob_smote = rf_smote.predict_proba(X_test)[:, 1]

print("\nSMOTE Results:")
evaluator_smote = ImbalancedEvaluator(y_test, y_pred_smote, y_prob_smote)
smote_metrics = evaluator_smote.print_detailed_report()
```

### Combined Sampling Methods

```python
class CombinedSamplingMethods:
    """Combine over and undersampling techniques"""

    def __init__(self):
        pass

    def smoteenn_combined(self, X, y):
        """SMOTE + Edited Nearest Neighbours"""

        try:
            from imblearn.combine import SMOTEENN

            smote_enn = SMOTEENN(random_state=42)
            X_resampled, y_resampled = smote_enn.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After SMOTE+ENN: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Using basic combination")
            return self.basic_combination(X, y)

    def smotetomek_combined(self, X, y):
        """SMOTE + Tomek Links"""

        try:
            from imblearn.combine import SMOTETomek

            smote_tomek = SMOTETomek(random_state=42)
            X_resampled, y_resampled = smote_tomek.fit_resample(X, y)

            print(f"Original distribution: {np.bincount(y)}")
            print(f"After SMOTE+Tomek: {np.bincount(y_resampled)}")

            return X_resampled, y_resampled

        except ImportError:
            print("imblearn not available. Using basic combination")
            return self.basic_combination(X, y)

    def basic_combination(self, X, y):
        """Basic combination: SMOTE then random undersampling"""

        oversampler = OversamplingMethods()
        undersampler = UndersamplingMethods()

        # First apply SMOTE
        X_over, y_over = oversampler.smote_oversampling(X, y)

        # Then apply undersampling (but to a ratio like 2:1 instead of 1:1)
        majority_indices = np.where(y_over == 0)[0]
        minority_indices = np.where(y_over == 1)[0]

        # Keep 2x majority samples compared to minority
        target_majority_size = len(minority_indices) * 2

        if len(majority_indices) > target_majority_size:
            np.random.seed(42)
            majority_downsampled = np.random.choice(
                majority_indices,
                size=target_majority_size,
                replace=False
            )

            combined_indices = np.concatenate([majority_downsampled, minority_indices])
            X_combined = X_over[combined_indices]
            y_combined = y_over[combined_indices]
        else:
            X_combined, y_combined = X_over, y_over

        print(f"After combination: {np.bincount(y_combined)}")

        return X_combined, y_combined

# Test combined methods
combined_sampler = CombinedSamplingMethods()

print("=== SMOTE + TOMEK COMBINED ===")
X_train_combined, y_train_combined = combined_sampler.smotetomek_combined(X_train, y_train)

# Train and evaluate
rf_combined = RandomForestClassifier(n_estimators=100, random_state=42)
rf_combined.fit(X_train_combined, y_train_combined)
y_pred_combined = rf_combined.predict(X_test)
y_prob_combined = rf_combined.predict_proba(X_test)[:, 1]

print("\nCombined Sampling Results:")
evaluator_combined = ImbalancedEvaluator(y_test, y_pred_combined, y_prob_combined)
combined_metrics = evaluator_combined.print_detailed_report()
```

## Cost-Sensitive Learning

Instead of changing the data, modify the algorithm to account for class importance:

### Class Weights and Cost-Sensitive Algorithms

```python
class CostSensitiveLearning:
    """Cost-sensitive approaches for imbalanced learning"""

    def __init__(self):
        pass

    def balanced_class_weights(self, X_train, y_train):
        """Use balanced class weights"""

        # Calculate balanced class weights
        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weight_dict = dict(zip(classes, weights))

        print(f"Balanced class weights: {class_weight_dict}")

        # Train models with balanced weights
        models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                class_weight='balanced',
                random_state=42
            ),
            'Logistic Regression': LogisticRegression(
                class_weight='balanced',
                random_state=42,
                max_iter=1000
            ),
            'SVM': SVC(
                class_weight='balanced',
                probability=True,
                random_state=42
            )
        }

        results = {}

        for name, model in models.items():
            print(f"\nTraining {name} with balanced weights...")

            # Use scaled features for SVM and Logistic Regression
            if name in ['SVM', 'Logistic Regression']:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)

                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                y_prob = model.predict_proba(X_test_scaled)[:, 1]
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                y_prob = model.predict_proba(X_test)[:, 1]

            # Evaluate
            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()

            results[name] = {
                'model': model,
                'metrics': metrics,
                'y_pred': y_pred,
                'y_prob': y_prob
            }

            print(f"F1-Score: {metrics['f1']:.4f}")
            print(f"Recall: {metrics['recall']:.4f}")
            print(f"Precision: {metrics['precision']:.4f}")

        return results

    def custom_class_weights(self, X_train, y_train, weight_ratios):
        """Custom class weight ratios"""

        results = {}

        for ratio_name, weights in weight_ratios.items():
            print(f"\n=== Custom Weights: {ratio_name} ===")
            print(f"Weight ratio (0:1) = {weights}")

            rf = RandomForestClassifier(
                n_estimators=100,
                class_weight=weights,
                random_state=42
            )

            rf.fit(X_train, y_train)
            y_pred = rf.predict(X_test)
            y_prob = rf.predict_proba(X_test)[:, 1]

            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()

            results[ratio_name] = {
                'weights': weights,
                'metrics': metrics,
                'y_pred': y_pred,
                'y_prob': y_prob
            }

            print(f"F1-Score: {metrics['f1']:.4f}")
            print(f"Recall: {metrics['recall']:.4f}")
            print(f"Precision: {metrics['precision']:.4f}")

        return results

    def cost_sensitive_svm(self, X_train, y_train):
        """Cost-sensitive SVM with custom misclassification costs"""

        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Different cost ratios
        cost_ratios = [1, 5, 10, 20, 50]
        results = {}

        for cost_ratio in cost_ratios:
            class_weights = {0: 1, 1: cost_ratio}

            svm = SVC(
                class_weight=class_weights,
                probability=True,
                random_state=42
            )

            svm.fit(X_train_scaled, y_train)
            y_pred = svm.predict(X_test_scaled)
            y_prob = svm.predict_proba(X_test_scaled)[:, 1]

            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()

            results[cost_ratio] = metrics

            print(f"Cost ratio 1:{cost_ratio} - F1: {metrics['f1']:.4f}, Recall: {metrics['recall']:.4f}")

        return results

# Test cost-sensitive learning
cost_sensitive = CostSensitiveLearning()

print("=== BALANCED CLASS WEIGHTS ===")
balanced_results = cost_sensitive.balanced_class_weights(X_train, y_train)

print("\n=== CUSTOM CLASS WEIGHTS ===")
custom_weight_ratios = {
    'Conservative': {0: 1, 1: 5},      # 1:5 ratio
    'Moderate': {0: 1, 1: 10},         # 1:10 ratio
    'Aggressive': {0: 1, 1: 20},       # 1:20 ratio
    'Extreme': {0: 1, 1: 50}           # 1:50 ratio
}

custom_results = cost_sensitive.custom_class_weights(X_train, y_train, custom_weight_ratios)

# Visualize trade-offs
weight_names = list(custom_results.keys())
f1_scores = [custom_results[name]['metrics']['f1'] for name in weight_names]
recalls = [custom_results[name]['metrics']['recall'] for name in weight_names]
precisions = [custom_results[name]['metrics']['precision'] for name in weight_names]

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.bar(weight_names, f1_scores)
plt.title('F1-Score by Weight Ratio')
plt.ylabel('F1-Score')
plt.xticks(rotation=45)

plt.subplot(1, 3, 2)
plt.bar(weight_names, recalls, color='coral')
plt.title('Recall by Weight Ratio')
plt.ylabel('Recall')
plt.xticks(rotation=45)

plt.subplot(1, 3, 3)
plt.bar(weight_names, precisions, color='lightgreen')
plt.title('Precision by Weight Ratio')
plt.ylabel('Precision')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()
```

## Ensemble Methods for Imbalanced Data

Specialized ensemble techniques designed for imbalanced datasets:

### Balanced Ensemble Methods

```python
class BalancedEnsembles:
    """Ensemble methods specifically designed for imbalanced data"""

    def __init__(self):
        pass

    def balanced_random_forest(self, X_train, y_train):
        """Balanced Random Forest with bootstrap sampling"""

        try:
            from imblearn.ensemble import BalancedRandomForestClassifier

            brf = BalancedRandomForestClassifier(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )

            brf.fit(X_train, y_train)
            y_pred = brf.predict(X_test)
            y_prob = brf.predict_proba(X_test)[:, 1]

            print("Balanced Random Forest Results:")
            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()
            evaluator.print_detailed_report()

            return brf, metrics

        except ImportError:
            print("imblearn not available. Using class-weighted Random Forest")
            return self.class_weighted_ensemble(X_train, y_train)

    def balanced_bagging(self, X_train, y_train):
        """Balanced Bagging Classifier"""

        try:
            from imblearn.ensemble import BalancedBaggingClassifier

            bbc = BalancedBaggingClassifier(
                base_estimator=None,  # Uses DecisionTreeClassifier by default
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )

            bbc.fit(X_train, y_train)
            y_pred = bbc.predict(X_test)
            y_prob = bbc.predict_proba(X_test)[:, 1]

            print("Balanced Bagging Results:")
            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()
            evaluator.print_detailed_report()

            return bbc, metrics

        except ImportError:
            print("imblearn not available. Using alternative ensemble")
            return self.class_weighted_ensemble(X_train, y_train)

    def easy_ensemble(self, X_train, y_train):
        """EasyEnsemble Classifier"""

        try:
            from imblearn.ensemble import EasyEnsembleClassifier

            eec = EasyEnsembleClassifier(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )

            eec.fit(X_train, y_train)
            y_pred = eec.predict(X_test)
            y_prob = eec.predict_proba(X_test)[:, 1]

            print("EasyEnsemble Results:")
            evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
            metrics = evaluator.compute_all_metrics()
            evaluator.print_detailed_report()

            return eec, metrics

        except ImportError:
            print("imblearn not available. Using alternative ensemble")
            return self.class_weighted_ensemble(X_train, y_train)

    def class_weighted_ensemble(self, X_train, y_train):
        """Fallback: Class-weighted ensemble"""

        rf = RandomForestClassifier(
            n_estimators=100,
            class_weight='balanced',
            random_state=42
        )

        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_test)
        y_prob = rf.predict_proba(X_test)[:, 1]

        print("Class-Weighted Random Forest Results:")
        evaluator = ImbalancedEvaluator(y_test, y_pred, y_prob)
        metrics = evaluator.compute_all_metrics()
        evaluator.print_detailed_report()

        return rf, metrics

# Test balanced ensemble methods
balanced_ensembles = BalancedEnsembles()

print("=== BALANCED RANDOM FOREST ===")
brf_model, brf_metrics = balanced_ensembles.balanced_random_forest(X_train, y_train)

print("\n=== BALANCED BAGGING ===")
bb_model, bb_metrics = balanced_ensembles.balanced_bagging(X_train, y_train)
```

## Threshold Optimization

Fine-tune the decision threshold for optimal performance:

### Threshold Tuning Methods

```python
class ThresholdOptimization:
    """Optimize decision thresholds for imbalanced classification"""

    def __init__(self):
        pass

    def find_optimal_threshold(self, y_true, y_prob, metric='f1'):
        """Find optimal threshold based on different metrics"""

        thresholds = np.linspace(0, 1, 101)
        scores = []

        for threshold in thresholds:
            y_pred_thresh = (y_prob >= threshold).astype(int)

            if metric == 'f1':
                score = f1_score(y_true, y_pred_thresh)
            elif metric == 'precision':
                score = precision_score(y_true, y_pred_thresh, zero_division=0)
            elif metric == 'recall':
                score = recall_score(y_true, y_pred_thresh, zero_division=0)
            elif metric == 'balanced_accuracy':
                score = balanced_accuracy_score(y_true, y_pred_thresh)
            elif metric == 'g_mean':
                tn, fp, fn, tp = confusion_matrix(y_true, y_pred_thresh).ravel()
                sensitivity = tp / (tp + fn)
                specificity = tn / (tn + fp)
                score = np.sqrt(sensitivity * specificity)

            scores.append(score)

        # Find best threshold
        best_idx = np.argmax(scores)
        best_threshold = thresholds[best_idx]
        best_score = scores[best_idx]

        return best_threshold, best_score, thresholds, scores

    def comprehensive_threshold_analysis(self, y_true, y_prob):
        """Analyze thresholds across multiple metrics"""

        metrics = ['f1', 'precision', 'recall', 'balanced_accuracy', 'g_mean']
        results = {}

        plt.figure(figsize=(15, 10))

        for i, metric in enumerate(metrics):
            best_thresh, best_score, thresholds, scores = self.find_optimal_threshold(
                y_true, y_prob, metric
            )

            results[metric] = {
                'best_threshold': best_thresh,
                'best_score': best_score,
                'all_scores': scores
            }

            # Plot
            plt.subplot(2, 3, i + 1)
            plt.plot(thresholds, scores)
            plt.axvline(best_thresh, color='red', linestyle='--',
                       label=f'Best: {best_thresh:.3f} ({best_score:.3f})')
            plt.xlabel('Threshold')
            plt.ylabel(metric.replace('_', ' ').title())
            plt.title(f'{metric.replace("_", " ").title()} vs Threshold')
            plt.legend()
            plt.grid(True, alpha=0.3)

            print(f"{metric}: Best threshold = {best_thresh:.3f}, Score = {best_score:.3f}")

        # Summary plot
        plt.subplot(2, 3, 6)
        thresholds_list = [results[m]['best_threshold'] for m in metrics]
        scores_list = [results[m]['best_score'] for m in metrics]

        bars = plt.bar(metrics, scores_list)
        plt.ylabel('Best Score')
        plt.title('Best Scores by Metric')
        plt.xticks(rotation=45)

        # Annotate thresholds
        for bar, thresh in zip(bars, thresholds_list):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{thresh:.3f}', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.show()

        return results

    def precision_recall_threshold_curve(self, y_true, y_prob):
        """Plot Precision-Recall curve with threshold information"""

        precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, y_prob)

        # Calculate F1 scores for each threshold
        f1_scores = 2 * (precision_curve[:-1] * recall_curve[:-1]) / (precision_curve[:-1] + recall_curve[:-1])

        # Find best F1 threshold
        best_f1_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_f1_idx]
        best_f1 = f1_scores[best_f1_idx]

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        # Precision-Recall curve
        axes[0].plot(recall_curve, precision_curve, linewidth=2)
        axes[0].scatter(recall_curve[best_f1_idx], precision_curve[best_f1_idx],
                       color='red', s=100, zorder=5,
                       label=f'Best F1 (threshold={best_threshold:.3f})')
        axes[0].set_xlabel('Recall')
        axes[0].set_ylabel('Precision')
        axes[0].set_title('Precision-Recall Curve')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Precision and Recall vs Threshold
        axes[1].plot(thresholds, precision_curve[:-1], label='Precision', linewidth=2)
        axes[1].plot(thresholds, recall_curve[:-1], label='Recall', linewidth=2)
        axes[1].axvline(best_threshold, color='red', linestyle='--',
                       label=f'Best F1 threshold: {best_threshold:.3f}')
        axes[1].set_xlabel('Threshold')
        axes[1].set_ylabel('Score')
        axes[1].set_title('Precision & Recall vs Threshold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        # F1 Score vs Threshold
        axes[2].plot(thresholds, f1_scores, linewidth=2, color='green')
        axes[2].axvline(best_threshold, color='red', linestyle='--',
                       label=f'Best F1: {best_f1:.3f}')
        axes[2].axhline(best_f1, color='red', linestyle='--', alpha=0.5)
        axes[2].set_xlabel('Threshold')
        axes[2].set_ylabel('F1 Score')
        axes[2].set_title('F1 Score vs Threshold')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        return best_threshold, best_f1

# Test threshold optimization using SMOTE model
threshold_optimizer = ThresholdOptimization()

print("=== THRESHOLD OPTIMIZATION ANALYSIS ===")
threshold_results = threshold_optimizer.comprehensive_threshold_analysis(y_test, y_prob_smote)

print("\n=== PRECISION-RECALL THRESHOLD ANALYSIS ===")
best_f1_threshold, best_f1_score = threshold_optimizer.precision_recall_threshold_curve(y_test, y_prob_smote)

# Apply best F1 threshold
y_pred_optimized = (y_prob_smote >= best_f1_threshold).astype(int)

print(f"\nOptimized Threshold Results (threshold={best_f1_threshold:.3f}):")
evaluator_optimized = ImbalancedEvaluator(y_test, y_pred_optimized, y_prob_smote)
optimized_metrics = evaluator_optimized.print_detailed_report()
```

## Comprehensive Method Comparison

Let's compare all approaches systematically:

```python
def comprehensive_imbalance_comparison(X_train, y_train, X_test, y_test):
    """Compare all imbalanced learning approaches"""

    methods = {}

    print("COMPREHENSIVE IMBALANCED LEARNING COMPARISON")
    print("=" * 60)

    # 1. Baseline (no handling)
    print("1. Training Baseline Model...")
    baseline_rf = RandomForestClassifier(n_estimators=100, random_state=42)
    baseline_rf.fit(X_train, y_train)
    y_pred_baseline = baseline_rf.predict(X_test)
    y_prob_baseline = baseline_rf.predict_proba(X_test)[:, 1]

    baseline_evaluator = ImbalancedEvaluator(y_test, y_pred_baseline, y_prob_baseline)
    baseline_metrics = baseline_evaluator.compute_all_metrics()
    methods['Baseline'] = baseline_metrics

    # 2. Random Undersampling
    print("2. Training with Random Undersampling...")
    undersampler = UndersamplingMethods()
    X_train_rus, y_train_rus = undersampler.random_undersampling(X_train, y_train)

    rf_rus = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_rus.fit(X_train_rus, y_train_rus)
    y_pred_rus = rf_rus.predict(X_test)
    y_prob_rus = rf_rus.predict_proba(X_test)[:, 1]

    rus_evaluator = ImbalancedEvaluator(y_test, y_pred_rus, y_prob_rus)
    rus_metrics = rus_evaluator.compute_all_metrics()
    methods['Random Undersampling'] = rus_metrics

    # 3. SMOTE
    print("3. Training with SMOTE...")
    oversampler = OversamplingMethods()
    X_train_smote, y_train_smote = oversampler.smote_oversampling(X_train, y_train)

    rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_smote.fit(X_train_smote, y_train_smote)
    y_pred_smote = rf_smote.predict(X_test)
    y_prob_smote = rf_smote.predict_proba(X_test)[:, 1]

    smote_evaluator = ImbalancedEvaluator(y_test, y_pred_smote, y_prob_smote)
    smote_metrics = smote_evaluator.compute_all_metrics()
    methods['SMOTE'] = smote_metrics

    # 4. Class Weights
    print("4. Training with Balanced Class Weights...")
    rf_weighted = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
    rf_weighted.fit(X_train, y_train)
    y_pred_weighted = rf_weighted.predict(X_test)
    y_prob_weighted = rf_weighted.predict_proba(X_test)[:, 1]

    weighted_evaluator = ImbalancedEvaluator(y_test, y_pred_weighted, y_prob_weighted)
    weighted_metrics = weighted_evaluator.compute_all_metrics()
    methods['Class Weights'] = weighted_metrics

    # 5. Threshold Optimization (using SMOTE model)
    print("5. Optimizing Decision Threshold...")
    threshold_opt = ThresholdOptimization()
    best_threshold, _, _, _ = threshold_opt.find_optimal_threshold(y_test, y_prob_smote, 'f1')
    y_pred_thresh = (y_prob_smote >= best_threshold).astype(int)

    thresh_evaluator = ImbalancedEvaluator(y_test, y_pred_thresh, y_prob_smote)
    thresh_metrics = thresh_evaluator.compute_all_metrics()
    methods['SMOTE + Threshold Opt'] = thresh_metrics

    # Create comprehensive comparison
    comparison_df = pd.DataFrame(methods).T

    # Select key metrics for comparison
    key_metrics = ['f1', 'recall', 'precision', 'balanced_accuracy', 'g_mean', 'auc_roc']
    comparison_summary = comparison_df[key_metrics]

    print("\nCOMPARISON SUMMARY:")
    print("=" * 50)
    print(comparison_summary.round(4))

    # Visualize comparison
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()

    for i, metric in enumerate(key_metrics):
        values = comparison_summary[metric].values
        methods_list = comparison_summary.index.tolist()

        bars = axes[i].bar(range(len(methods_list)), values)
        axes[i].set_xticks(range(len(methods_list)))
        axes[i].set_xticklabels(methods_list, rotation=45, ha='right')
        axes[i].set_ylabel(metric.replace('_', ' ').title())
        axes[i].set_title(f'{metric.replace("_", " ").title()} Comparison')
        axes[i].grid(True, alpha=0.3)

        # Highlight best method
        best_idx = np.argmax(values)
        bars[best_idx].set_color('gold')

        # Add value labels
        for bar, val in zip(bars, values):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{val:.3f}', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.show()

    # Rank methods
    print("\nMETHOD RANKINGS:")
    print("=" * 30)

    for metric in key_metrics:
        sorted_methods = comparison_summary.sort_values(metric, ascending=False)
        print(f"\n{metric.replace('_', ' ').title()}:")
        for rank, (method, score) in enumerate(sorted_methods[metric].items(), 1):
            print(f"  {rank}. {method}: {score:.4f}")

    return comparison_df

# Run comprehensive comparison
final_comparison = comprehensive_imbalance_comparison(X_train, y_train, X_test, y_test)
```

## Best Practices and Guidelines

### Decision Framework for Imbalanced Data

```python
def imbalanced_data_decision_framework():
    """Decision framework for choosing imbalanced learning techniques"""

    framework = {
        "Dataset Size": {
            "Small (<1000 samples)": [
                "Use stratified sampling",
                "Prefer oversampling (SMOTE) over undersampling",
                "Consider class weights",
                "Be careful with cross-validation"
            ],
            "Medium (1K-100K samples)": [
                "Try both over and undersampling",
                "SMOTE + ENN combination works well",
                "Class weights are effective",
                "Use stratified k-fold CV"
            ],
            "Large (>100K samples)": [
                "Undersampling is computationally efficient",
                "Class weights scale well",
                "Ensemble methods are powerful",
                "Consider computational constraints"
            ]
        },

        "Imbalance Ratio": {
            "Mild (1:4 to 1:10)": [
                "Class weights often sufficient",
                "Threshold optimization effective",
                "Standard algorithms may work",
                "Monitor precision-recall trade-off"
            ],
            "Moderate (1:10 to 1:100)": [
                "SMOTE or other synthetic methods",
                "Combine with undersampling",
                "Ensemble methods recommended",
                "Focus on recall for minority class"
            ],
            "Severe (>1:100)": [
                "Advanced synthetic generation (ADASYN)",
                "Anomaly detection approaches",
                "Cost-sensitive learning",
                "One-class classification"
            ]
        },

        "Problem Domain": {
            "Medical Diagnosis": [
                "High recall priority (catch all positives)",
                "Cost-sensitive learning",
                "Conservative thresholds",
                "Interpretable models preferred"
            ],
            "Fraud Detection": [
                "Balance precision and recall",
                "Real-time constraints important",
                "Ensemble methods effective",
                "Regular model updates needed"
            ],
            "Manufacturing QC": [
                "Minimize false positives (cost)",
                "Seasonal patterns matter",
                "Domain-specific features",
                "Threshold optimization crucial"
            ]
        },

        "Model Requirements": {
            "Interpretability Needed": [
                "Simple models with class weights",
                "Threshold optimization",
                "Avoid complex ensembles",
                "Feature importance analysis"
            ],
            "Maximum Performance": [
                "Ensemble methods",
                "Multiple resampling techniques",
                "Hyperparameter optimization",
                "Model stacking"
            ],
            "Real-time Deployment": [
                "Efficient algorithms",
                "Pre-computed thresholds",
                "Avoid complex preprocessing",
                "Monitor data drift"
            ]
        }
    }

    print("IMBALANCED DATA DECISION FRAMEWORK")
    print("=" * 50)

    for category, subcategories in framework.items():
        print(f"\nðŸŽ¯ {category}:")
        for subcat, recommendations in subcategories.items():
            print(f"\n  ðŸ“‹ {subcat}:")
            for rec in recommendations:
                print(f"    â€¢ {rec}")

    return framework

def recommend_approach(dataset_size, imbalance_ratio, domain, requirements):
    """Get personalized recommendations"""

    recommendations = []

    # Based on dataset size
    if dataset_size < 1000:
        recommendations.extend([
            "Use SMOTE for oversampling",
            "Apply class weights to algorithms",
            "Use stratified cross-validation"
        ])
    elif dataset_size < 100000:
        recommendations.extend([
            "Try SMOTE + ENN combination",
            "Compare with class weights approach",
            "Use ensemble methods"
        ])
    else:
        recommendations.extend([
            "Consider undersampling for efficiency",
            "Use balanced ensemble methods",
            "Class weights scale well"
        ])

    # Based on imbalance ratio
    if imbalance_ratio > 100:
        recommendations.extend([
            "Severe imbalance detected",
            "Use advanced synthetic methods (ADASYN)",
            "Consider anomaly detection approaches"
        ])
    elif imbalance_ratio > 10:
        recommendations.extend([
            "Moderate imbalance",
            "SMOTE or synthetic oversampling",
            "Focus on minority class recall"
        ])
    else:
        recommendations.extend([
            "Mild imbalance",
            "Class weights may be sufficient",
            "Threshold optimization effective"
        ])

    # Based on domain
    if domain == "medical":
        recommendations.extend([
            "Prioritize high recall (sensitivity)",
            "Use conservative thresholds",
            "Consider cost of false negatives"
        ])
    elif domain == "fraud":
        recommendations.extend([
            "Balance precision and recall",
            "Use ensemble methods",
            "Plan for model updates"
        ])

    print("PERSONALIZED RECOMMENDATIONS:")
    print("=" * 40)
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec}")

    return recommendations

# Display framework and get recommendations
decision_framework = imbalanced_data_decision_framework()

# Example recommendations
example_recommendations = recommend_approach(
    dataset_size=len(X_train),
    imbalance_ratio=19,  # 95:5 ratio
    domain="general",
    requirements="balanced"
)
```

## Conclusion

Handling imbalanced datasets is a critical skill in real-world machine learning. The key insights to remember:

### ðŸŽ¯ **Core Principles**
- **Accuracy is Misleading**: Focus on precision, recall, F1, and balanced accuracy
- **One Size Doesn't Fit All**: Different problems need different solutions
- **Evaluation is Critical**: Use appropriate metrics and cross-validation strategies
- **Business Context Matters**: Consider the real-world costs of different errors

### ðŸ”§ **Main Approaches**
- **Resampling**: SMOTE for oversampling, Tomek Links for undersampling
- **Cost-Sensitive**: Class weights and custom loss functions
- **Ensemble Methods**: Balanced Random Forest and specialized ensembles
- **Threshold Optimization**: Fine-tune decision boundaries for optimal trade-offs

### ðŸ“Š **Best Practices**
- **Start with Class Weights**: Simple and often effective first step
- **Combine Techniques**: Over/undersampling + threshold optimization
- **Stratified CV**: Maintain class distribution in cross-validation
- **Monitor Both Classes**: Don't optimize minority at expense of majority

### âš–ï¸ **Selection Guidelines**
- **Small Datasets**: SMOTE + class weights
- **Large Datasets**: Undersampling + ensemble methods
- **Severe Imbalance (>1:100)**: ADASYN + cost-sensitive learning
- **Real-time Systems**: Pre-computed thresholds + efficient algorithms

### ðŸŽ² **Advanced Techniques**
- **Synthetic Generation**: SMOTE, ADASYN, Borderline-SMOTE
- **Hybrid Methods**: SMOTE-ENN, SMOTE-Tomek combinations
- **Ensemble Approaches**: Balanced Random Forest, Easy Ensemble
- **Threshold Tuning**: Precision-recall curve optimization

Remember: imbalanced data is the norm, not the exception, in real-world problems. The key is to understand your specific context, choose appropriate evaluation metrics, and systematically test different approaches. Focus on the minority class performance while ensuring you don't completely sacrifice majority class accuracy. With proper techniques, you can build models that perform well on both classes! ðŸš€