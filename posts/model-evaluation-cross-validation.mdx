---
title: "Model Evaluation and Cross-Validation Techniques"
description: "Master model evaluation with train/validation/test splits, k-fold cross-validation, and comprehensive metrics. Learn different evaluation strategies with practical examples."
date: "2024-11-30"
author: "Tech Blogger"
tags: ["Machine Learning", "Model Evaluation", "Cross Validation"]
image: "/images/model-evaluation.jpg"
readTime: "8 min read"
---

# Model Evaluation and Cross-Validation Techniques

Model evaluation is the cornerstone of successful machine learning. Without proper evaluation, you can't distinguish between a model that truly understands patterns and one that has simply memorized training data. This comprehensive guide covers everything from basic train-test splits to advanced cross-validation strategies.

## Why Model Evaluation Matters

The fundamental challenge in machine learning is ensuring your model generalizes well to unseen data. Poor evaluation can lead to:

- **Overfitting**: High training performance, poor real-world performance
- **Data leakage**: Accidentally using future information to predict the past
- **Biased estimates**: Overconfident predictions that don't hold up
- **Poor model selection**: Choosing suboptimal algorithms or hyperparameters

```python
# The evaluation paradox
training_accuracy = 0.99  # Looks amazing!
real_world_accuracy = 0.65  # Reality check...

# Why this happens:
# 1. Model memorized training data
# 2. Training data isn't representative
# 3. Evaluation methodology was flawed
# 4. Data leakage occurred
```

## The Foundation: Train-Validation-Test Split

The gold standard for model evaluation involves three distinct datasets:

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.model_selection import GridSearchCV, cross_validate, learning_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Load sample dataset
from sklearn.datasets import make_classification

# Create a sample dataset
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_clusters_per_class=1,
    random_state=42
)

# Convert to DataFrame for easier handling
feature_names = [f'feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

print(f"Dataset shape: {df.shape}")
print(f"Class distribution: {np.bincount(y)}")
```

### Three-Way Split Strategy

```python
class DataSplitter:
    """Comprehensive data splitting with various strategies"""

    def __init__(self, X, y, test_size=0.2, val_size=0.2, random_state=42):
        self.X = X
        self.y = y
        self.test_size = test_size
        self.val_size = val_size
        self.random_state = random_state

    def basic_split(self):
        """Basic train-validation-test split"""

        # First split: separate test set
        X_temp, X_test, y_temp, y_test = train_test_split(
            self.X, self.y,
            test_size=self.test_size,
            random_state=self.random_state,
            stratify=self.y  # Maintain class distribution
        )

        # Second split: separate train and validation from remaining data
        val_size_adjusted = self.val_size / (1 - self.test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size_adjusted,
            random_state=self.random_state,
            stratify=y_temp
        )

        return {
            'X_train': X_train, 'y_train': y_train,
            'X_val': X_val, 'y_val': y_val,
            'X_test': X_test, 'y_test': y_test
        }

    def temporal_split(self, time_column):
        """Time-based split for temporal data"""

        # Sort by time
        sorted_indices = np.argsort(time_column)

        n_samples = len(sorted_indices)
        train_end = int(n_samples * (1 - self.test_size - self.val_size))
        val_end = int(n_samples * (1 - self.test_size))

        train_idx = sorted_indices[:train_end]
        val_idx = sorted_indices[train_end:val_end]
        test_idx = sorted_indices[val_end:]

        return {
            'X_train': self.X[train_idx], 'y_train': self.y[train_idx],
            'X_val': self.X[val_idx], 'y_val': self.y[val_idx],
            'X_test': self.X[test_idx], 'y_test': self.y[test_idx]
        }

    def stratified_group_split(self, groups):
        """Split while maintaining group integrity and stratification"""
        from sklearn.model_selection import GroupShuffleSplit

        # This is a simplified version - real implementation would be more complex
        gss = GroupShuffleSplit(n_splits=1, test_size=self.test_size, random_state=self.random_state)

        train_val_idx, test_idx = next(gss.split(self.X, self.y, groups))

        # Further split train_val into train and val
        X_temp, y_temp, groups_temp = self.X[train_val_idx], self.y[train_val_idx], groups[train_val_idx]

        val_size_adjusted = self.val_size / (1 - self.test_size)
        gss_val = GroupShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=self.random_state)

        train_idx, val_idx = next(gss_val.split(X_temp, y_temp, groups_temp))

        return {
            'X_train': X_temp[train_idx], 'y_train': y_temp[train_idx],
            'X_val': X_temp[val_idx], 'y_val': y_temp[val_idx],
            'X_test': self.X[test_idx], 'y_test': self.y[test_idx]
        }

# Example usage
splitter = DataSplitter(X, y)
data_splits = splitter.basic_split()

print("Split sizes:")
for split_name, data in data_splits.items():
    if 'X_' in split_name:
        print(f"{split_name}: {data.shape[0]} samples")

# Visualize class distribution across splits
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
splits = ['train', 'val', 'test']

for i, split in enumerate(splits):
    y_split = data_splits[f'y_{split}']
    class_counts = np.bincount(y_split)

    axes[i].bar(['Class 0', 'Class 1'], class_counts)
    axes[i].set_title(f'{split.capitalize()} Set Distribution')
    axes[i].set_ylabel('Count')

    # Add percentage labels
    total = sum(class_counts)
    for j, count in enumerate(class_counts):
        axes[i].text(j, count + 5, f'{count/total:.1%}', ha='center')

plt.tight_layout()
plt.show()
```

## Cross-Validation: The Gold Standard

Cross-validation provides more robust performance estimates by using multiple train-validation splits:

### K-Fold Cross-Validation

```python
class CrossValidator:
    """Comprehensive cross-validation implementation"""

    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.cv_results = {}

    def k_fold_cv(self, model, k=5, scoring='accuracy', random_state=42):
        """Standard K-Fold Cross-Validation"""

        from sklearn.model_selection import KFold

        kfold = KFold(n_splits=k, shuffle=True, random_state=random_state)

        scores = cross_val_score(model, self.X, self.y, cv=kfold, scoring=scoring)

        self.cv_results['kfold'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': f'{k}-Fold'
        }

        return scores

    def stratified_k_fold_cv(self, model, k=5, scoring='accuracy', random_state=42):
        """Stratified K-Fold Cross-Validation for imbalanced datasets"""

        skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)

        scores = cross_val_score(model, self.X, self.y, cv=skfold, scoring=scoring)

        self.cv_results['stratified_kfold'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': f'Stratified {k}-Fold'
        }

        return scores

    def leave_one_out_cv(self, model, scoring='accuracy'):
        """Leave-One-Out Cross-Validation"""

        from sklearn.model_selection import LeaveOneOut

        loo = LeaveOneOut()
        scores = cross_val_score(model, self.X, self.y, cv=loo, scoring=scoring)

        self.cv_results['loo'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': 'Leave-One-Out'
        }

        return scores

    def time_series_cv(self, model, n_splits=5, scoring='accuracy'):
        """Time Series Cross-Validation"""

        from sklearn.model_selection import TimeSeriesSplit

        tscv = TimeSeriesSplit(n_splits=n_splits)
        scores = cross_val_score(model, self.X, self.y, cv=tscv, scoring=scoring)

        self.cv_results['time_series'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': 'Time Series'
        }

        return scores

    def group_k_fold_cv(self, model, groups, k=5, scoring='accuracy'):
        """Group K-Fold Cross-Validation"""

        from sklearn.model_selection import GroupKFold

        gkfold = GroupKFold(n_splits=k)
        scores = cross_val_score(model, self.X, self.y, groups=groups, cv=gkfold, scoring=scoring)

        self.cv_results['group_kfold'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': f'Group {k}-Fold'
        }

        return scores

    def repeated_k_fold_cv(self, model, k=5, n_repeats=3, scoring='accuracy', random_state=42):
        """Repeated K-Fold Cross-Validation for more stable estimates"""

        from sklearn.model_selection import RepeatedStratifiedKFold

        rskfold = RepeatedStratifiedKFold(
            n_splits=k,
            n_repeats=n_repeats,
            random_state=random_state
        )

        scores = cross_val_score(model, self.X, self.y, cv=rskfold, scoring=scoring)

        self.cv_results['repeated_kfold'] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std(),
            'cv_type': f'Repeated {k}-Fold ({n_repeats} repeats)'
        }

        return scores

# Example usage
model = RandomForestClassifier(n_estimators=100, random_state=42)
cv = CrossValidator(X, y)

# Different CV strategies
print("Cross-Validation Results:")
print("=" * 50)

k_fold_scores = cv.k_fold_cv(model, k=5)
print(f"K-Fold (5): {k_fold_scores.mean():.4f} ± {k_fold_scores.std():.4f}")

stratified_scores = cv.stratified_k_fold_cv(model, k=5)
print(f"Stratified K-Fold (5): {stratified_scores.mean():.4f} ± {stratified_scores.std():.4f}")

repeated_scores = cv.repeated_k_fold_cv(model, k=5, n_repeats=3)
print(f"Repeated Stratified K-Fold: {repeated_scores.mean():.4f} ± {repeated_scores.std():.4f}")

# Visualize CV results
plt.figure(figsize=(12, 6))
cv_methods = ['kfold', 'stratified_kfold', 'repeated_kfold']
positions = range(1, len(cv_methods) + 1)

for i, method in enumerate(cv_methods):
    if method in cv.cv_results:
        scores = cv.cv_results[method]['scores']
        plt.boxplot(scores, positions=[positions[i]], widths=0.3)
        plt.scatter([positions[i]] * len(scores), scores, alpha=0.6)

plt.xticks(positions, [cv.cv_results[method]['cv_type'] for method in cv_methods], rotation=45)
plt.ylabel('Accuracy Score')
plt.title('Cross-Validation Method Comparison')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Advanced Cross-Validation Techniques

```python
def nested_cross_validation(X, y, model, param_grid, outer_cv=5, inner_cv=3, scoring='accuracy'):
    """Nested CV for unbiased model selection and evaluation"""

    from sklearn.model_selection import GridSearchCV

    outer_scores = []
    best_params_list = []

    outer_skfold = StratifiedKFold(n_splits=outer_cv, shuffle=True, random_state=42)

    for fold, (train_idx, test_idx) in enumerate(outer_skfold.split(X, y)):
        print(f"Processing outer fold {fold + 1}/{outer_cv}")

        X_train_outer, X_test_outer = X[train_idx], X[test_idx]
        y_train_outer, y_test_outer = y[train_idx], y[test_idx]

        # Inner CV for hyperparameter selection
        inner_skfold = StratifiedKFold(n_splits=inner_cv, shuffle=True, random_state=42)

        grid_search = GridSearchCV(
            model, param_grid, cv=inner_skfold,
            scoring=scoring, n_jobs=-1, verbose=0
        )

        grid_search.fit(X_train_outer, y_train_outer)
        best_params_list.append(grid_search.best_params_)

        # Test on outer fold
        best_model = grid_search.best_estimator_
        outer_score = best_model.score(X_test_outer, y_test_outer)
        outer_scores.append(outer_score)

    return {
        'outer_scores': outer_scores,
        'mean_score': np.mean(outer_scores),
        'std_score': np.std(outer_scores),
        'best_params': best_params_list
    }

# Example with hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10]
}

rf_model = RandomForestClassifier(random_state=42)
nested_results = nested_cross_validation(X, y, rf_model, param_grid, outer_cv=3, inner_cv=3)

print("Nested Cross-Validation Results:")
print(f"Mean Score: {nested_results['mean_score']:.4f} ± {nested_results['std_score']:.4f}")
print("Best Parameters per Fold:")
for i, params in enumerate(nested_results['best_params']):
    print(f"  Fold {i+1}: {params}")
```

## Comprehensive Model Evaluation Metrics

Different problems require different evaluation metrics. Here's a comprehensive toolkit:

### Classification Metrics

```python
class ClassificationEvaluator:
    """Comprehensive classification evaluation"""

    def __init__(self, y_true, y_pred, y_pred_proba=None, labels=None):
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_pred_proba = y_pred_proba
        self.labels = labels
        self.n_classes = len(np.unique(y_true))

    def basic_metrics(self):
        """Calculate basic classification metrics"""

        metrics = {
            'accuracy': accuracy_score(self.y_true, self.y_pred),
            'precision_macro': precision_score(self.y_true, self.y_pred, average='macro'),
            'precision_micro': precision_score(self.y_true, self.y_pred, average='micro'),
            'precision_weighted': precision_score(self.y_true, self.y_pred, average='weighted'),
            'recall_macro': recall_score(self.y_true, self.y_pred, average='macro'),
            'recall_micro': recall_score(self.y_true, self.y_pred, average='micro'),
            'recall_weighted': recall_score(self.y_true, self.y_pred, average='weighted'),
            'f1_macro': f1_score(self.y_true, self.y_pred, average='macro'),
            'f1_micro': f1_score(self.y_true, self.y_pred, average='micro'),
            'f1_weighted': f1_score(self.y_true, self.y_pred, average='weighted'),
        }

        return metrics

    def confusion_matrix_analysis(self):
        """Detailed confusion matrix analysis"""

        cm = confusion_matrix(self.y_true, self.y_pred)

        # Calculate per-class metrics
        per_class_metrics = {}
        for i in range(self.n_classes):
            tp = cm[i, i]
            fp = cm[:, i].sum() - tp
            fn = cm[i, :].sum() - tp
            tn = cm.sum() - tp - fp - fn

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            per_class_metrics[f'class_{i}'] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'support': cm[i, :].sum()
            }

        return cm, per_class_metrics

    def roc_auc_analysis(self):
        """ROC-AUC analysis for binary and multiclass"""

        if self.y_pred_proba is None:
            print("Need prediction probabilities for ROC-AUC analysis")
            return None

        if self.n_classes == 2:
            # Binary classification
            fpr, tpr, thresholds = roc_curve(self.y_true, self.y_pred_proba[:, 1])
            auc_score = roc_auc_score(self.y_true, self.y_pred_proba[:, 1])

            return {
                'fpr': fpr,
                'tpr': tpr,
                'thresholds': thresholds,
                'auc': auc_score
            }
        else:
            # Multiclass classification
            from sklearn.preprocessing import label_binarize
            from sklearn.metrics import roc_curve, auc

            y_bin = label_binarize(self.y_true, classes=range(self.n_classes))

            fpr, tpr, auc_scores = {}, {}, {}

            for i in range(self.n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], self.y_pred_proba[:, i])
                auc_scores[i] = auc(fpr[i], tpr[i])

            # Compute micro-average ROC curve
            fpr['micro'], tpr['micro'], _ = roc_curve(y_bin.ravel(), self.y_pred_proba.ravel())
            auc_scores['micro'] = auc(fpr['micro'], tpr['micro'])

            return {
                'fpr': fpr,
                'tpr': tpr,
                'auc_scores': auc_scores
            }

    def plot_evaluation_results(self):
        """Create comprehensive evaluation plots"""

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Confusion Matrix
        cm, _ = self.confusion_matrix_analysis()
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('Confusion Matrix')
        axes[0, 0].set_xlabel('Predicted Label')
        axes[0, 0].set_ylabel('True Label')

        # 2. ROC Curve (if probabilities available)
        if self.y_pred_proba is not None:
            roc_data = self.roc_auc_analysis()

            if self.n_classes == 2:
                axes[0, 1].plot(roc_data['fpr'], roc_data['tpr'],
                               label=f'ROC Curve (AUC = {roc_data["auc"]:.3f})')
                axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)
            else:
                for i in range(min(3, self.n_classes)):  # Show first 3 classes
                    axes[0, 1].plot(roc_data['fpr'][i], roc_data['tpr'][i],
                                   label=f'Class {i} (AUC = {roc_data["auc_scores"][i]:.3f})')
                axes[0, 1].plot(roc_data['fpr']['micro'], roc_data['tpr']['micro'],
                               label=f'Micro-avg (AUC = {roc_data["auc_scores"]["micro"]:.3f})',
                               linestyle='--')

            axes[0, 1].set_xlabel('False Positive Rate')
            axes[0, 1].set_ylabel('True Positive Rate')
            axes[0, 1].set_title('ROC Curve')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. Metrics Comparison
        metrics = self.basic_metrics()
        metric_names = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']
        metric_values = [metrics[name] for name in metric_names]

        axes[1, 0].bar(range(len(metric_names)), metric_values)
        axes[1, 0].set_xticks(range(len(metric_names)))
        axes[1, 0].set_xticklabels([name.replace('_', ' ').title() for name in metric_names], rotation=45)
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title('Performance Metrics')
        axes[1, 0].set_ylim(0, 1)

        # Add value labels on bars
        for i, v in enumerate(metric_values):
            axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

        # 4. Classification Report Heatmap
        report = classification_report(self.y_true, self.y_pred, output_dict=True)

        # Convert to DataFrame for heatmap
        report_df = pd.DataFrame(report).iloc[:-1, :-3].T  # Remove support and averages

        sns.heatmap(report_df, annot=True, cmap='YlOrRd', ax=axes[1, 1])
        axes[1, 1].set_title('Classification Report')

        plt.tight_layout()
        plt.show()

# Example usage
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Get predictions
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)

# Evaluate
evaluator = ClassificationEvaluator(y_test, y_pred, y_pred_proba)

# Get metrics
basic_metrics = evaluator.basic_metrics()
print("Basic Classification Metrics:")
for metric, value in basic_metrics.items():
    print(f"{metric}: {value:.4f}")

# Plot results
evaluator.plot_evaluation_results()
```

### Regression Metrics

```python
class RegressionEvaluator:
    """Comprehensive regression evaluation"""

    def __init__(self, y_true, y_pred):
        self.y_true = y_true
        self.y_pred = y_pred
        self.residuals = y_true - y_pred

    def basic_metrics(self):
        """Calculate basic regression metrics"""

        metrics = {
            'mae': mean_absolute_error(self.y_true, self.y_pred),
            'mse': mean_squared_error(self.y_true, self.y_pred),
            'rmse': np.sqrt(mean_squared_error(self.y_true, self.y_pred)),
            'r2': r2_score(self.y_true, self.y_pred),
            'mape': np.mean(np.abs((self.y_true - self.y_pred) / self.y_true)) * 100,
            'max_error': np.max(np.abs(self.residuals))
        }

        return metrics

    def advanced_metrics(self):
        """Calculate advanced regression metrics"""

        # Mean Absolute Percentage Error (handling zero values)
        def safe_mape(y_true, y_pred):
            mask = y_true != 0
            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

        # Symmetric Mean Absolute Percentage Error
        def smape(y_true, y_pred):
            return np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))) * 100

        # Explained Variance Score
        from sklearn.metrics import explained_variance_score

        advanced_metrics = {
            'safe_mape': safe_mape(self.y_true, self.y_pred),
            'smape': smape(self.y_true, self.y_pred),
            'explained_variance': explained_variance_score(self.y_true, self.y_pred),
            'residual_std': np.std(self.residuals),
            'residual_mean': np.mean(self.residuals)
        }

        return advanced_metrics

    def residual_analysis(self):
        """Comprehensive residual analysis"""

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. Residuals vs Predicted
        axes[0, 0].scatter(self.y_pred, self.residuals, alpha=0.6)
        axes[0, 0].axhline(y=0, color='r', linestyle='--')
        axes[0, 0].set_xlabel('Predicted Values')
        axes[0, 0].set_ylabel('Residuals')
        axes[0, 0].set_title('Residuals vs Predicted')
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Q-Q Plot
        stats.probplot(self.residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Q-Q Plot of Residuals')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Histogram of Residuals
        axes[1, 0].hist(self.residuals, bins=30, density=True, alpha=0.7, color='skyblue')
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].set_title('Distribution of Residuals')
        axes[1, 0].grid(True, alpha=0.3)

        # Add normal distribution overlay
        mu, sigma = np.mean(self.residuals), np.std(self.residuals)
        x = np.linspace(self.residuals.min(), self.residuals.max(), 100)
        axes[1, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal')
        axes[1, 0].legend()

        # 4. Predicted vs Actual
        axes[1, 1].scatter(self.y_true, self.y_pred, alpha=0.6)
        min_val = min(self.y_true.min(), self.y_pred.min())
        max_val = max(self.y_true.max(), self.y_pred.max())
        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
        axes[1, 1].set_xlabel('Actual Values')
        axes[1, 1].set_ylabel('Predicted Values')
        axes[1, 1].set_title('Predicted vs Actual')
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

# Example for regression (create sample regression data)
from sklearn.datasets import make_regression

X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# Train regression model
from sklearn.ensemble import RandomForestRegressor
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train_reg, y_train_reg)
y_pred_reg = rf_reg.predict(X_test_reg)

# Evaluate
reg_evaluator = RegressionEvaluator(y_test_reg, y_pred_reg)
reg_metrics = reg_evaluator.basic_metrics()

print("Regression Metrics:")
for metric, value in reg_metrics.items():
    print(f"{metric}: {value:.4f}")

reg_evaluator.residual_analysis()
```

## Learning Curves and Model Diagnosis

Learning curves help diagnose overfitting, underfitting, and data size requirements:

```python
def plot_learning_curves(model, X, y, train_sizes=np.linspace(0.1, 1.0, 10),
                        cv=5, scoring='accuracy', title="Learning Curves"):
    """Plot learning curves to diagnose model performance"""

    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, train_sizes=train_sizes, cv=cv,
        scoring=scoring, n_jobs=-1, random_state=42
    )

    # Calculate means and standard deviations
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    # Plot
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                     alpha=0.2, color='blue')

    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Cross-Validation Score')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,
                     alpha=0.2, color='red')

    plt.xlabel('Training Set Size')
    plt.ylabel('Score')
    plt.title(f'{title}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Diagnosis
    plt.subplot(1, 2, 2)
    gap = train_mean - val_mean
    plt.plot(train_sizes, gap, 'o-', color='green', label='Training-Validation Gap')
    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    plt.xlabel('Training Set Size')
    plt.ylabel('Performance Gap')
    plt.title('Overfitting Diagnosis')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Diagnosis text
    final_gap = gap[-1]
    final_val_score = val_mean[-1]

    print("Learning Curve Diagnosis:")
    print("=" * 30)

    if final_gap > 0.1:
        print("🔴 HIGH OVERFITTING detected")
        print("   - Training score much higher than validation score")
        print("   - Consider: regularization, more data, simpler model")
    elif final_gap > 0.05:
        print("🟡 MODERATE OVERFITTING detected")
        print("   - Some gap between training and validation scores")
        print("   - Consider: slight regularization, more data")
    else:
        print("🟢 LOW OVERFITTING - Good generalization")

    if final_val_score < 0.7:  # Adjust threshold as needed
        print("🔴 UNDERFITTING suspected")
        print("   - Both training and validation scores are low")
        print("   - Consider: more complex model, more features")

    if train_mean[-1] - train_mean[0] > 0.1:
        print("📈 Model benefits from more training data")
    else:
        print("📊 Model performance plateaued - more data may not help")

# Example usage
models_to_evaluate = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42, probability=True)
}

for name, model in models_to_evaluate.items():
    print(f"\n{'='*50}")
    print(f"Learning Curves for {name}")
    print('='*50)
    plot_learning_curves(model, X, y, title=f"{name} Learning Curves")
```

## Statistical Significance Testing

Determine if performance differences between models are statistically significant:

```python
def compare_models_statistical(models_dict, X, y, cv=5, alpha=0.05):
    """Compare models with statistical significance testing"""

    from scipy import stats
    from itertools import combinations

    # Collect CV scores for each model
    model_scores = {}

    for name, model in models_dict.items():
        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
        model_scores[name] = scores
        print(f"{name}: {scores.mean():.4f} ± {scores.std():.4f}")

    print("\nStatistical Significance Tests:")
    print("=" * 40)

    # Pairwise comparisons
    model_names = list(model_scores.keys())

    for model1, model2 in combinations(model_names, 2):
        scores1 = model_scores[model1]
        scores2 = model_scores[model2]

        # Paired t-test
        t_stat, p_value = stats.ttest_rel(scores1, scores2)

        print(f"\n{model1} vs {model2}:")
        print(f"  Mean difference: {scores1.mean() - scores2.mean():.4f}")
        print(f"  t-statistic: {t_stat:.4f}")
        print(f"  p-value: {p_value:.4f}")

        if p_value < alpha:
            winner = model1 if scores1.mean() > scores2.mean() else model2
            print(f"  🎯 Significant difference (p < {alpha})")
            print(f"  🏆 {winner} is significantly better")
        else:
            print(f"  ❌ No significant difference (p >= {alpha})")

    return model_scores

# Example usage
models_comparison = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42)
}

model_comparison_results = compare_models_statistical(models_comparison, X, y, cv=5)
```

## Best Practices for Model Evaluation

### 1. **Choose the Right Cross-Validation Strategy**

```python
def recommend_cv_strategy(dataset_size, n_classes, has_groups=False, is_time_series=False):
    """Recommend appropriate CV strategy based on data characteristics"""

    recommendations = []

    if is_time_series:
        recommendations.append("Use TimeSeriesSplit to respect temporal ordering")
    elif has_groups:
        recommendations.append("Use GroupKFold to prevent group leakage")
    elif dataset_size < 100:
        recommendations.append("Use LeaveOneOut CV due to small dataset")
    elif dataset_size < 1000:
        recommendations.append("Use 5-fold or 10-fold CV")
    else:
        recommendations.append("Use 5-fold CV (sufficient for large datasets)")

    # Class imbalance check
    if n_classes > 1:
        recommendations.append("Use StratifiedKFold to maintain class distribution")

    # Stability recommendation
    if dataset_size > 1000:
        recommendations.append("Consider RepeatedKFold for more stable estimates")

    return recommendations

# Usage
recommendations = recommend_cv_strategy(
    dataset_size=len(X),
    n_classes=len(np.unique(y)),
    has_groups=False,
    is_time_series=False
)

print("Recommended CV Strategy:")
for i, rec in enumerate(recommendations, 1):
    print(f"{i}. {rec}")
```

### 2. **Evaluation Checklist**

```python
def model_evaluation_checklist(model, X, y, model_name="Model"):
    """Comprehensive model evaluation checklist"""

    checklist = {
        "Data Split": False,
        "Cross-Validation": False,
        "Multiple Metrics": False,
        "Confusion Matrix": False,
        "Learning Curves": False,
        "Feature Importance": False,
        "Residual Analysis": False,  # For regression
        "Statistical Significance": False
    }

    print(f"Model Evaluation Checklist for {model_name}")
    print("=" * 50)

    # 1. Data Split
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        checklist["Data Split"] = True
        print("✅ Data properly split into train/test sets")
    except:
        print("❌ Failed to split data")

    # 2. Cross-Validation
    try:
        cv_scores = cross_val_score(model, X, y, cv=5)
        checklist["Cross-Validation"] = True
        print(f"✅ Cross-validation completed: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")
    except:
        print("❌ Cross-validation failed")

    # 3. Multiple Metrics
    try:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        if hasattr(model, "predict_proba"):  # Classification
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'f1': f1_score(y_test, y_pred, average='weighted'),
                'precision': precision_score(y_test, y_pred, average='weighted'),
                'recall': recall_score(y_test, y_pred, average='weighted')
            }
        else:  # Regression
            metrics = {
                'mae': mean_absolute_error(y_test, y_pred),
                'mse': mean_squared_error(y_test, y_pred),
                'r2': r2_score(y_test, y_pred)
            }

        checklist["Multiple Metrics"] = True
        print("✅ Multiple evaluation metrics calculated")

    except:
        print("❌ Failed to calculate metrics")

    # 4. Feature Importance (if available)
    try:
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            checklist["Feature Importance"] = True
            print("✅ Feature importance analyzed")
        else:
            print("ℹ️  Feature importance not available for this model")
    except:
        print("❌ Failed to analyze feature importance")

    print(f"\nChecklist Completion: {sum(checklist.values())}/{len(checklist)} items completed")

    return checklist

# Example usage
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
evaluation_results = model_evaluation_checklist(rf_model, X, y, "Random Forest")
```

## Conclusion

Effective model evaluation is the foundation of reliable machine learning. The key principles to remember:

### 🎯 **Core Principles**
- **Always use proper train-validation-test splits**
- **Cross-validation provides more robust estimates**
- **Choose metrics appropriate for your problem**
- **Test for statistical significance**

### 📊 **Essential Practices**
- **Stratified sampling** for imbalanced datasets
- **Time-aware splits** for temporal data
- **Group-aware splits** to prevent data leakage
- **Multiple metrics** for comprehensive evaluation

### 🔍 **Advanced Techniques**
- **Nested cross-validation** for hyperparameter tuning
- **Learning curves** for diagnosing overfitting
- **Residual analysis** for regression problems
- **Statistical testing** for model comparison

### ⚠️ **Common Pitfalls to Avoid**
- Using test set for model selection (data leakage)
- Relying on single metrics (accuracy bias)
- Ignoring class imbalance in CV splits
- Not accounting for temporal dependencies

Remember: evaluation is not a one-time activity but an iterative process. Good evaluation practices will save you from deploying models that look great in development but fail in production. The time invested in thorough evaluation always pays off in more reliable and trustworthy models! 🚀